{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tPS2Qv9jPfs"
   },
   "source": [
    "# Preprocess Data \n",
    "create .npy stacks of train/test/val data from raster data (from GEE) to use to train model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0XYJoeijSmp"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHSUwlPQdta4",
    "outputId": "b8b0c8ed-e5da-4b42-928f-d90171637765"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from datetime import datetime,date\n",
    "import numpy as np\n",
    "import calendar\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "# from shapely.geometry import box\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers import Dropout, Dense, LSTM, RepeatVector, TimeDistributed, Input, Reshape, BatchNormalization\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# import pickle\n",
    "\n",
    "# import tensorflow as tf #version 2.11.0\n",
    "# import keras #v 2.11.0\n",
    "import os\n",
    "\n",
    "# import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "# from rasterio.warp import calculate_default_transform#, reproject\n",
    "\n",
    "# import json\n",
    "\n",
    "from math import sqrt\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "      \n",
    "np.random.seed(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysGHWGyVlauO"
   },
   "source": [
    "## Define Functions to Read in Raster and Point Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rM6-3S_lcrf"
   },
   "outputs": [],
   "source": [
    "\n",
    "def turn_raster_output_to_gdf(dynamic_stack_fp, points, band_names_stack, prefire_months=120):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts raster values at given points within a bounding box.\n",
    "\n",
    "     Args:\n",
    "    [str] dynamic_Stack_fp = Path to the raster file (gtiff file.\n",
    "    [GeoDataFrame] points = point shapefile /Points to extract raster values from\n",
    "    [list of str] band_names_stack = names of attributes to add to output gdf\n",
    "    [int] prefire months = number of months before the fire occurred that we are interested in/ \n",
    "                            Number of prefire months for time series matching.\n",
    "\n",
    "     Creates:\n",
    "     [list] values (list): List of extracted values and corresponding points.\n",
    "     \n",
    "     \n",
    "     Then..Turns output from the extract_points function into a geodataframe\n",
    "\n",
    "     Returns: \n",
    "     [geodataframe] Extracted values from raster and geometry points in dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raster = rio.open(dynamic_stack_fp)\n",
    "\n",
    "    values = []\n",
    "\n",
    "    point_num = len(points)#_clip)\n",
    "    array = raster.read()\n",
    "\n",
    "    # Loop through points, get the value of the raster at each xy\n",
    "    for point in points['geometry']:\n",
    "  \n",
    "        x, y = point.xy[0][0], point.xy[1][0]\n",
    "        row, col = raster.index(x, y)\n",
    "        attrs = []\n",
    "        band_nums = array.shape[0]\n",
    "\n",
    "        for z in range(band_nums):\n",
    "                attrs.append(array[z, row, col])\n",
    "           \n",
    "        values.append([attrs, point])\n",
    "        \n",
    "    # Close raster\n",
    "    raster.close()\n",
    "    \n",
    "    #convert values to dataframe\n",
    "    output_pd = pd.DataFrame(values)\n",
    "    \n",
    "    lis = output_pd[0].to_list() #list of attributes in format [precip1, temp1, ndvi1, precip2, temp2, ndvi2...]\n",
    "    geom = output_pd[1].to_list() #geometry list (of point coords)\n",
    "    # Turn output into labeled\n",
    "    org = pd.DataFrame(lis, columns = band_names_stack)\n",
    "    # read points into geodataframe\n",
    "    return gpd.GeoDataFrame(org, geometry=geom, crs=points.crs)\n",
    "\n",
    "\n",
    "def turn_raster_output_to_gdf_STATIC(dynamic_stack_fp, points, band_names_stack, prefire_months=120):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts raster values at given points within a bounding box.\n",
    "    For use with static rasters(e.g. slope (which doesnt change) instead of NDVI which is monthly)\n",
    "    Difference from non-static version of function is that this passes if the point isn't located\n",
    "    within a given raster tile bc these rasters are *multipart*\n",
    "\n",
    "     Args:\n",
    "    [str] dynamic_Stack_fp = Path to the raster file (gtiff file)\n",
    "    [GeoDataFrame] points = point shapefile /Points to extract raster values from\n",
    "    [str list] band_names_stack = names of attributes to add to output gdf\n",
    "    [int] prefire months = number of months before the fire occurred that we are interested in/ \n",
    "                            Number of prefire months for time series matching.\n",
    "\n",
    "     Creates:\n",
    "     [list] values (list): List of extracted values and corresponding points.\n",
    "     \n",
    "     Then..Turns output from the extract_points function into a geodataframe\n",
    "\n",
    "     Returns: \n",
    "     [geodataframe] Extracted values from raster and geometry points in dataframe\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raster = rio.open(dynamic_stack_fp)\n",
    "    \n",
    "    #first ensure that raster and points are same crs:\n",
    "\n",
    "    if raster.crs != points.crs:\n",
    "        print(\"NEED TO REPROJECT FIRST! Cannot continue..\")\n",
    "        print(raster.crs,'is raster crs; points crs is:', points.crs)\n",
    "        out_path = dynamic_stack_fp.replace('.tif','_'+str(points.crs).replace('epsg:','')+'.tif')\n",
    "        reproject_raster(dynamic_stack_fp, points.crs, out_path)#output_raster_path='default')\n",
    "#         raise\n",
    "        raster = rio.open(out_path)\n",
    "    \n",
    "    values = []\n",
    "    point_num = len(points)#_clip)\n",
    "    array = raster.read()\n",
    "    \n",
    "    # Loop through points, get the value of the raster at each xy\n",
    "    for point in points['geometry']:\n",
    "        x, y = point.xy[0][0], point.xy[1][0]\n",
    "#         print(\"x,y\",x,y)\n",
    "        row, col = raster.index(x, y)\n",
    "#         print(\"r,c\",row,col)\n",
    "        attrs = []\n",
    "        band_nums = array.shape[0]\n",
    "\n",
    "        for z in range(band_nums):\n",
    "            try:\n",
    "                if np.isnan(array[z, row, col]) and z!=4: #when z=4, this is severity where there was an issue... all are nan (basically)\n",
    "                    raise\n",
    "                else:\n",
    "#                 if not np.isnan(array[z, row, col]):\n",
    "                #!= True and array[z, row, col] != 'nan' and array[z, row, col] is not None:\\\n",
    "#                     print(type(array[z,row,col]), array[z,row,col]) #np.float; e.g 9.0\n",
    "                    attrs.append([array[z, row, col]] * prefire_months) #ERROR HERE\n",
    "            \n",
    "#                 else:\n",
    "#                     print('val:',array[z, row, col])\n",
    "#                     fake_app = -999999#[-999999 for i in range(prefire_months)]\n",
    "#                     attrs.append(fake_app)\n",
    "#                     values.append([attrs, point])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "        if len(attrs)>0:\n",
    "            values.append([attrs, point]) ## in other function this is unindented to same as 'for z in range..'\n",
    "            \n",
    "    # Close raster\n",
    "    raster.close()\n",
    "    \n",
    "        \n",
    "    output_pd = pd.DataFrame(values)\n",
    "    \n",
    "    lis = output_pd[0].to_list() #list of attributes in format [precip1, temp1, ndvi1, precip2, temp2, ndvi2...]\n",
    "    geom = output_pd[1].to_list() #geometry list (of point coords)\n",
    "#     print(len(geom),'is len geom')\n",
    "#     print(\"lis 0:\",len(lis), len(lis[0]), len(lis[0][0]))\n",
    "    # Turn output into labeled\n",
    "    org = pd.DataFrame(lis, columns = band_names_stack)\n",
    "    # read points into geodataframe\n",
    "    return gpd.GeoDataFrame(org, geometry=geom, crs=points.crs)\n",
    "\n",
    "def reproject_raster(input_raster_path, target_crs, output_raster_path='default'):\n",
    "    \"\"\"\n",
    "    Reproject a raster file to a new coordinate reference system (CRS).\n",
    "\n",
    "    Args:\n",
    "        input_raster_path (str): The file path to the input raster file.\n",
    "        target_crs (rasterio.crs.CRS): The target CRS to reproject the raster to.\n",
    "        output_raster_path (str): The file path to the output raster file. Default is 'default'.\n",
    "    \n",
    "    Returns:\n",
    "        None (saves the reprojected raster to the output_raster_path)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    src = rio.open(input_raster_path)\n",
    "\n",
    "\n",
    "    # Create an output raster file\n",
    "    dst = rio.open(output_raster_path, 'w', driver='GTiff',\n",
    "                        height=src.height, width=src.width,\n",
    "                        count=src.count, dtype=src.dtypes[0],\n",
    "                        crs=target_crs, transform=src.transform)\n",
    "\n",
    "    # Loop through each band and perform the reprojection\n",
    "    for band_idx in range(1, src.count + 1):\n",
    "        reproject( #using rasterio.warp/\n",
    "            \n",
    "            source=rio.band(src, band_idx),\n",
    "            destination=rio.band(dst, band_idx),\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=dst.transform,\n",
    "            dst_crs=dst.crs,\n",
    "            dst_nodata=np.nan,\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "    # Close the files\n",
    "    src.close()\n",
    "    dst.close()\n",
    "\n",
    "\n",
    "def convert_epoch_milli_to_datetime(milli):\n",
    "  return(datetime.fromtimestamp(int(milli/1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWcd7GskjhyW"
   },
   "source": [
    "## Read In & Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List filepaths to data on local machine (laptop) and set output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list filepaths to data\n",
    "\n",
    "vcf_tree_fp = #tif of vcf pct tree per year\n",
    "lai_fp = # tif for modis monthly LAI\n",
    "geojson_fp = #points geojson (regularly sampled within fire boundary)\n",
    "dynamic_stack_fp = #geotif of dynamic (precip, temp, ndvi)\n",
    "dyn_stack_extracted_pts_fp = #shapefile of points\n",
    "fire_bounds_fp = #fire bounds shapefile\n",
    "static_fp = #data folder for static rasters\n",
    "static_fp1 = #static rasters area tiled; reference part 1\n",
    "static_fp2 = \n",
    "static_fp3 = \n",
    "static_fp4 = \n",
    "burn_main_dir = # directory with burn severity rasters for each fire\n",
    "\n",
    "#name output file path where to save all data:\n",
    "out_fp = #create new home dir \n",
    "    os.mkdir(out_fp)\n",
    "os.chdir(out_fp)\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nyj2Q8Q-hRf"
   },
   "source": [
    "### Read in Point geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VFtZRvVeEnf",
    "outputId": "3dc5381c-460c-4de0-fbd3-f3ea064019b1"
   },
   "outputs": [],
   "source": [
    "# Load in shapefile of points to extract from raster\n",
    "points_shp = gpd.read_file(geojson_fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbDTt1Uv-mu_"
   },
   "source": [
    "### Load in dynamic raster stack, rename bands, extract points from stack to create gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vVGVmHSCaBz"
   },
   "outputs": [],
   "source": [
    "# Read in time series raster stack\n",
    "\n",
    "# Create band names list, created to match the order the bands come from google earth engine\n",
    "band_names_stack = []\n",
    "\n",
    "years = list(range(2000, 2022))\n",
    "months = list(calendar.month_name[1:])\n",
    "vars = ['temp','precip',  'NDVI']\n",
    "\n",
    "for year in years:\n",
    "  for month in months:\n",
    "    for var in vars:\n",
    "      band_names_stack.append(var + month[:3] + str(year))\n",
    "      #note: NDVI data from Jan 2000 is a copy of Feb 2000 (as NDVI MODIS data doesnt exist for jan) but this month won't get used as there arent fires from jan 2000 so its ok\n",
    "\n",
    "# Load in raster time series stack and extract points from the loaded in raster\n",
    "# ras_id = '1_WtM8diGPc-swcu9xMFm0Bczqd4fFV1F' ##updated 7/19 '1J_BVg2o4rabBgXLJRJVKw1CvZO23hm7H'\n",
    "# fdsafdsafdsa this raster is the old one with errors! switch to new one which is being generated in gee in cs17 on 7/11\n",
    "\n",
    "time_series_gdf = turn_raster_output_to_gdf(dynamic_stack_fp, points_shp, band_names_stack, 120)\n",
    "time_series_gdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1RDV2MpIVKh"
   },
   "source": [
    "### Divide all numeric columns by 100\n",
    "purpose: to revert them back to original data values\n",
    "(in gee I multiplied everythign by 100 so I could convert to int32 without losing precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "ow1i11rLqZjM",
    "outputId": "38da0aff-ff48-44ed-c632-cca932783a37"
   },
   "outputs": [],
   "source": [
    "#remove ML Stack to free up ram\n",
    "# os.remove('MLstack.tif')\n",
    "print(\"make sure to check dtype of this array in case its in int and doesnt coorectly change to float\")\n",
    "\n",
    "#modify in place\n",
    "\n",
    "# Select only the numeric columns (excluding the geometry column)\n",
    "numeric_cols = time_series_gdf.select_dtypes(include=[pd.np.number]).columns\n",
    "\n",
    "# Divide all numeric columns by 100\n",
    "time_series_gdf[numeric_cols] = time_series_gdf[numeric_cols] / 100\n",
    "\n",
    "time_series_gdf.head()\n",
    "#NOTE: true min and max of MODIS NDVI 250m 16day product are: -2,000 and\t10,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssf3RQJGINK0",
    "outputId": "857a4d56-6eb4-4fe8-d7da-788bfb807bdd"
   },
   "outputs": [],
   "source": [
    "gdf_dtype = time_series_gdf[time_series_gdf.columns[1]].dtype\n",
    "print(gdf_dtype, 'is datatype of geodataframe')\n",
    "if gdf_dtype != 'float32' and gdf_dtype != 'float64':\n",
    "  print(\"need to convert to float rather than int \")#\"(though first check this datatype after dividing by 100 to seee if it converts automatically)\")\n",
    "  raise\n",
    "time_series_gdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6ykLHG-DzBx"
   },
   "source": [
    "### Read in LAI Data (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpAKbx6BDyZ1",
    "outputId": "b4864dc3-50f3-408e-ec02-f3f2ea3fe604"
   },
   "outputs": [],
   "source": [
    "# lai_id = r'1Q4G3-ypzEfthwJzprHD1tT5irA2AXL0h'\n",
    "\n",
    "# print(time_series_gdf)\n",
    "lai_band_names = ['Band 1: Feb00', 'Band 2: Mar00', 'Band 3: Apr00', 'Band 4: May00', 'Band 5: Jun00', 'Band 6: Jul00', 'Band 7: Aug00', 'Band 8: Sep00', 'Band 9: Oct00', 'Band 10: Nov00', 'Band 11: Dec00', 'Band 12: Jan01', 'Band 13: Feb01', 'Band 14: Mar01', 'Band 15: Apr01', 'Band 16: May01', 'Band 17: Jun01', 'Band 18: Jul01', 'Band 19: Aug01', 'Band 20: Sep01', 'Band 21: Oct01', 'Band 22: Nov01', 'Band 23: Dec01', 'Band 24: Jan02', 'Band 25: Feb02', 'Band 26: Mar02', 'Band 27: Apr02', 'Band 28: May02', 'Band 29: Jun02', 'Band 30: Jul02', 'Band 31: Aug02', 'Band 32: Sep02', 'Band 33: Oct02', 'Band 34: Nov02', 'Band 35: Dec02', 'Band 36: Jan03', 'Band 37: Feb03', 'Band 38: Mar03', 'Band 39: Apr03', 'Band 40: May03', 'Band 41: Jun03', 'Band 42: Jul03', 'Band 43: Aug03', 'Band 44: Sep03', 'Band 45: Oct03', 'Band 46: Nov03', 'Band 47: Dec03', 'Band 48: Jan04', 'Band 49: Feb04', 'Band 50: Mar04', 'Band 51: Apr04', 'Band 52: May04', 'Band 53: Jun04', 'Band 54: Jul04', 'Band 55: Aug04', 'Band 56: Sep04', 'Band 57: Oct04', 'Band 58: Nov04', 'Band 59: Dec04', 'Band 60: Jan05', 'Band 61: Feb05', 'Band 62: Mar05', 'Band 63: Apr05', 'Band 64: May05', 'Band 65: Jun05', 'Band 66: Jul05', 'Band 67: Aug05', 'Band 68: Sep05', 'Band 69: Oct05', 'Band 70: Nov05', 'Band 71: Dec05', 'Band 72: Jan06', 'Band 73: Feb06', 'Band 74: Mar06', 'Band 75: Apr06', 'Band 76: May06', 'Band 77: Jun06', 'Band 78: Jul06', 'Band 79: Aug06', 'Band 80: Sep06', 'Band 81: Oct06', 'Band 82: Nov06', 'Band 83: Dec06', 'Band 84: Jan07', 'Band 85: Feb07', 'Band 86: Mar07', 'Band 87: Apr07', 'Band 88: May07', 'Band 89: Jun07', 'Band 90: Jul07', 'Band 91: Aug07', 'Band 92: Sep07', 'Band 93: Oct07', 'Band 94: Nov07', 'Band 95: Dec07', 'Band 96: Jan08', 'Band 97: Feb08', 'Band 98: Mar08', 'Band 99: Apr08', 'Band 100: May08', 'Band 101: Jun08', 'Band 102: Jul08', 'Band 103: Aug08', 'Band 104: Sep08', 'Band 105: Oct08', 'Band 106: Nov08', 'Band 107: Dec08', 'Band 108: Jan09', 'Band 109: Feb09', 'Band 110: Mar09', 'Band 111: Apr09', 'Band 112: May09', 'Band 113: Jun09', 'Band 114: Jul09', 'Band 115: Aug09', 'Band 116: Sep09', 'Band 117: Oct09', 'Band 118: Nov09', 'Band 119: Dec09', 'Band 120: Jan10', 'Band 121: Feb10', 'Band 122: Mar10', 'Band 123: Apr10', 'Band 124: May10', 'Band 125: Jun10', 'Band 126: Jul10', 'Band 127: Aug10', 'Band 128: Sep10', 'Band 129: Oct10', 'Band 130: Nov10', 'Band 131: Dec10', 'Band 132: Jan11', 'Band 133: Feb11', 'Band 134: Mar11', 'Band 135: Apr11', 'Band 136: May11', 'Band 137: Jun11', 'Band 138: Jul11', 'Band 139: Aug11', 'Band 140: Sep11', 'Band 141: Oct11', 'Band 142: Nov11', 'Band 143: Dec11', 'Band 144: Jan12', 'Band 145: Feb12', 'Band 146: Mar12', 'Band 147: Apr12', 'Band 148: May12', 'Band 149: Jun12', 'Band 150: Jul12', 'Band 151: Aug12', 'Band 152: Sep12', 'Band 153: Oct12', 'Band 154: Nov12', 'Band 155: Dec12', 'Band 156: Jan13', 'Band 157: Feb13', 'Band 158: Mar13', 'Band 159: Apr13', 'Band 160: May13', 'Band 161: Jun13', 'Band 162: Jul13', 'Band 163: Aug13', 'Band 164: Sep13', 'Band 165: Oct13', 'Band 166: Nov13', 'Band 167: Dec13', 'Band 168: Jan14', 'Band 169: Feb14', 'Band 170: Mar14', 'Band 171: Apr14', 'Band 172: May14', 'Band 173: Jun14', 'Band 174: Jul14', 'Band 175: Aug14', 'Band 176: Sep14', 'Band 177: Oct14', 'Band 178: Nov14', 'Band 179: Dec14', 'Band 180: Jan15', 'Band 181: Feb15', 'Band 182: Mar15', 'Band 183: Apr15', 'Band 184: May15', 'Band 185: Jun15', 'Band 186: Jul15', 'Band 187: Aug15', 'Band 188: Sep15', 'Band 189: Oct15', 'Band 190: Nov15', 'Band 191: Dec15', 'Band 192: Jan16', 'Band 193: Feb16', 'Band 194: Mar16', 'Band 195: Apr16', 'Band 196: May16', 'Band 197: Jun16', 'Band 198: Jul16', 'Band 199: Aug16', 'Band 200: Sep16', 'Band 201: Oct16', 'Band 202: Nov16', 'Band 203: Dec16', 'Band 204: Jan17', 'Band 205: Feb17', 'Band 206: Mar17', 'Band 207: Apr17', 'Band 208: May17', 'Band 209: Jun17', 'Band 210: Jul17', 'Band 211: Aug17', 'Band 212: Sep17', 'Band 213: Oct17', 'Band 214: Nov17', 'Band 215: Dec17', 'Band 216: Jan18', 'Band 217: Feb18', 'Band 218: Mar18', 'Band 219: Apr18', 'Band 220: May18', 'Band 221: Jun18', 'Band 222: Jul18', 'Band 223: Aug18', 'Band 224: Sep18', 'Band 225: Oct18', 'Band 226: Nov18', 'Band 227: Dec18', 'Band 228: Jan19', 'Band 229: Feb19', 'Band 230: Mar19', 'Band 231: Apr19', 'Band 232: May19', 'Band 233: Jun19', 'Band 234: Jul19', 'Band 235: Aug19', 'Band 236: Sep19', 'Band 237: Oct19', 'Band 238: Nov19', 'Band 239: Dec19', 'Band 240: Jan20', 'Band 241: Feb20', 'Band 242: Mar20', 'Band 243: Apr20', 'Band 244: May20', 'Band 245: Jun20', 'Band 246: Jul20', 'Band 247: Aug20', 'Band 248: Sep20', 'Band 249: Oct20', 'Band 250: Nov20', 'Band 251: Dec20', 'Band 252: Jan21', 'Band 253: Feb21', 'Band 254: Mar21', 'Band 255: Apr21', 'Band 256: May21', 'Band 257: Jun21', 'Band 258: Jul21', 'Band 259: Aug21', 'Band 260: Sep21', 'Band 261: Oct21', 'Band 262: Nov21', 'Band 263: Dec21']\n",
    "target_lai_band_names = ['LAI'+d[-5:] for d in lai_band_names]\n",
    "\n",
    "# for i in range(1,22):\n",
    "#   if len(str(i)) ==1:\n",
    "#     lai_band_names.append('Band 00'+ str(i)+\": \")\n",
    "#   elif len(str(i)) ==2:\n",
    "#     lai_band_names.append('Band 0'+ str(i)+\": \")\n",
    "\n",
    "time_series_gdf_lai = turn_raster_output_to_gdf(lai_fp, points_shp, band_names_stack=lai_band_names, prefire_months = 120)\n",
    "\n",
    "# rename bands\n",
    "renameLAI_dict = {i:j for i,j in zip(lai_band_names, target_lai_band_names)}\n",
    "time_series_gdf_lai = time_series_gdf_lai.rename(columns=renameLAI_dict)\n",
    "\n",
    "time_series_gdf_lai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXeBoQOMK4q9"
   },
   "source": [
    "### Read in VCF Data (annual) - to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgc5bUgJH51v"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vcf_tree_bandnames = ['PctTree00','PctTree01','PctTree02','PctTree03','PctTree04','PctTree05','PctTree06','PctTree07','PctTree08','PctTree09','PctTree10','PctTree11','PctTree12','PctTree13','PctTree14','PctTree15','PctTree16','PctTree17','PctTree18','PctTree19','PctTree20']\n",
    "time_series_gdf_vcfTree = turn_raster_output_to_gdf(vcf_tree_fp, points_shp, vcf_tree_bandnames, prefire_months = 120)\n",
    "time_series_gdf_vcfTree.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyCJ2FYANBxw"
   },
   "source": [
    "#### Preprocess VCF Tree Cover Pct DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9__nxiI-L4hY",
    "outputId": "abdf86db-923f-4809-edc7-9b8c8364e5ae"
   },
   "outputs": [],
   "source": [
    "#preprocess VCf Tree cover data\n",
    "\n",
    "# Assuming you have a Pandas GeoDataFrame called 'gdf'\n",
    "# Replace 'gdf' with the name of your GeoDataFrame\n",
    "\n",
    "# Dictionary to map month numbers to their corresponding names\n",
    "month_mapping = {\n",
    "    1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "    7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"\n",
    "}\n",
    "\n",
    "# List to store the columns to be deleted\n",
    "columns_to_delete = []\n",
    "\n",
    "# Loop through each column in the GeoDataFrame\n",
    "for column in time_series_gdf_vcfTree.columns:\n",
    "    # Check if the column name starts with \"PctTree\" (assuming all such columns are to be duplicated)\n",
    "    if column.startswith(\"PctTree\"):\n",
    "        # Get the year from the column name (assuming it is the last two characters of the column name)\n",
    "        year = column[-2:]\n",
    "\n",
    "        # Loop through each month (1 to 12) and create a new column for each\n",
    "        for month in range(1, 13):\n",
    "            # Create the new column name for the current year\n",
    "            new_column_name = column.replace(\"PctTree\", \"Tree\" + month_mapping[month])\n",
    "\n",
    "            # Copy the data from the original column to the new column for the current year\n",
    "            time_series_gdf_vcfTree[new_column_name] = time_series_gdf_vcfTree[column]\n",
    "\n",
    "        # If the current year is 2020, copy the data to the new column for the next year (2021)\n",
    "        if year == \"20\":\n",
    "\n",
    "            for month in range(1, 13):\n",
    "              # Create the new column name for the current year\n",
    "              new_column_name = column.replace(\"PctTree20\", \"Tree\" + month_mapping[month]+\"21\")\n",
    "\n",
    "              # Copy the data from the original column to the new column for the current year\n",
    "              time_series_gdf_vcfTree[new_column_name] = time_series_gdf_vcfTree[column]\n",
    "\n",
    "        # Add the original column to the list of columns to delete\n",
    "        columns_to_delete.append(column)\n",
    "\n",
    "# Delete the original columns\n",
    "time_series_gdf_vcfTree.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Now the GeoDataFrame 'gdf' will have the original columns duplicated 12 times with the new column names\n",
    "# \"TreeJan00\", \"TreeFeb00\", etc., and the original columns that started with \"PctTree\" will be deleted.\n",
    "# Additionally, the 2020 data will be copied into 12 additional columns for 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWK-6Hsj-yv0"
   },
   "source": [
    "### Read in fire boundary geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKBIrw_Z5cP2",
    "outputId": "79343021-0b1f-4170-c3ce-327c1b3da1dd"
   },
   "outputs": [],
   "source": [
    "# Read in the fire boundaries, converting the date into a readable format\n",
    "fire_bounds = gpd.read_file(fire_bounds_fp)#.to_crs(time_series_gdf.crs)#fireboundID, \"ForestFires_MLFinal.geojson\").to_crs(time_series_gdf.crs)\n",
    "print(\"CRS is equal:\",fire_bounds.crs==time_series_gdf.crs)\n",
    "fire_bounds['Ig_Date'] = fire_bounds.Ig_Date.apply(convert_epoch_milli_to_datetime)\n",
    "fire_bounds_dates = fire_bounds[['Ig_Date','Event_ID', 'geometry']]\n",
    "\n",
    "#check whether event ID is unique\n",
    "print(len(fire_bounds_dates['Event_ID']) == len(set(fire_bounds_dates['Event_ID'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vFveLOg-14I"
   },
   "source": [
    "### Add in the dates from the fire boundaries to the point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "TD9o2Igb7vyc",
    "outputId": "6232a97c-74df-4d05-9315-f9ff72321509"
   },
   "outputs": [],
   "source": [
    "# Add in the dates from the fire boundaries to the point data\n",
    "time_series_fire_date = gpd.sjoin(time_series_gdf, fire_bounds_dates, how='inner')\n",
    "#rename column to prepare for next sjoin\n",
    "time_series_fire_date = time_series_fire_date.rename(columns={'index_right': 'index_right1'})\n",
    "#join lai data frame with dynamic stack (ndiv, precip, temp) data frame\n",
    "time_series_fire_date = gpd.sjoin(time_series_fire_date, time_series_gdf_lai)\n",
    "time_series_fire_date = time_series_fire_date.rename(columns={'index_right': 'index_right2'})\n",
    "\n",
    "time_series_fire_date = gpd.sjoin(time_series_fire_date, time_series_gdf_vcfTree)\n",
    "\n",
    "time_series_fire_date = time_series_fire_date[~time_series_fire_date.index.duplicated(keep='first')]\n",
    "time_series_fire_date = time_series_fire_date.drop(['index_right'], axis=1)\n",
    "print(time_series_fire_date['Ig_Date'][0:2])\n",
    "time_series_fire_date.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po-PV85p-9uB"
   },
   "source": [
    "### Split gdf into variable groups for viz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iOMFbJvTumF"
   },
   "outputs": [],
   "source": [
    "# Split the dataframe into the variables currently stored in it\n",
    "selectThese =  [i.startswith('precip') or 'Ig_Date' in i or 'Event_ID' in i for i in time_series_fire_date.columns] #bool list\n",
    "precip = time_series_fire_date[time_series_fire_date.columns[selectThese]]\n",
    "\n",
    "selectThese =  [i.startswith('temp') or 'Ig_Date' in i  for i in time_series_fire_date.columns] #bool list\n",
    "temp = time_series_fire_date[time_series_fire_date.columns[selectThese]]\n",
    "\n",
    "selectThese =  [i.startswith('NDVI') or 'Ig_Date' in i  for i in time_series_fire_date.columns] #bool list\n",
    "ndvi = time_series_fire_date[time_series_fire_date.columns[selectThese]]\n",
    "\n",
    "selectThese =  [i.startswith('LAI') or 'Ig_Date' in i for i in time_series_fire_date.columns] #bool list\n",
    "lai = time_series_fire_date[time_series_fire_date.columns[selectThese]]\n",
    "\n",
    "\n",
    "selectThese =  [i.startswith('Tree') or 'Ig_Date' in i for i in time_series_fire_date.columns] #bool list\n",
    "vcf_tree = time_series_fire_date[time_series_fire_date.columns[selectThese]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1wATksOG3mP"
   },
   "outputs": [],
   "source": [
    "# Put the regular data frames into geodataframes so it can be visually spot checked\n",
    "precip_gdf = gpd.GeoDataFrame(precip, geometry=time_series_fire_date.geometry, crs=time_series_fire_date.crs)\n",
    "temp_gdf = gpd.GeoDataFrame(temp, geometry=time_series_fire_date.geometry, crs=time_series_fire_date.crs)\n",
    "ndvi_gdf = gpd.GeoDataFrame(ndvi, geometry=time_series_fire_date.geometry, crs=time_series_fire_date.crs)\n",
    "lai_gdf = gpd.GeoDataFrame(lai, geometry=time_series_fire_date.geometry, crs=time_series_fire_date.crs)\n",
    "vcf_tree_gdf = gpd.GeoDataFrame(vcf_tree, geometry=time_series_fire_date.geometry, crs=time_series_fire_date.crs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r6q8GBGjjp0"
   },
   "source": [
    "### Viz Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DJuq7QV5N1oE",
    "outputId": "b18aec9b-2dd9-4f25-94d8-d1329db8fb3a"
   },
   "outputs": [],
   "source": [
    "# Plot the data types and their spatial extents\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10))\n",
    "ax1, ax2, ax3, ax4 = axes[0][0], axes[0][1], axes[1][0], axes[1][1]\n",
    "ax1.set_title(\"precip\")\n",
    "ax2.set_title(\"temp\")\n",
    "ax3.set_title(\"ndvi\")\n",
    "precip_gdf.plot(column=\"precipJan2000\", ax=ax1, markersize=1)\n",
    "temp_gdf.plot(column=\"tempJan2000\", ax=ax2, markersize=1)\n",
    "ndvi_gdf.plot(column=\"NDVIJan2000\", ax=ax3, markersize=1)\n",
    "lai_gdf.plot(column=\"LAIFeb01\", ax=ax4, markersize=1)\n",
    "# id_gdf.plot(column='Post_ID', ax=ax4, markersize=1)\n",
    "for ax in axes:\n",
    "  for a in ax:\n",
    "    fire_bounds.boundary.plot(ax=a, color='black', linewidth=.5)\n",
    "fig.suptitle(\"Plot the first time stamp from each variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRe_4Xsp_ZMh"
   },
   "source": [
    "### Split data into model input  vs. output\n",
    "\n",
    "- input: pre-fire NDVI, precip, temp\n",
    "\n",
    "- output: post-fire NDVI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mW0Tn--0HqKY"
   },
   "outputs": [],
   "source": [
    "# Take the datasets created, loop through them, parse the date from the column\n",
    "# name, and then create a pandas time series for just that variable at each point\n",
    "\n",
    "#how many months after fire do we want to split model input vs. output?\n",
    "postfire_mo = 12\n",
    "prefire_mo = 120 + postfire_mo  #120 = 12 mo/year * 10 years\n",
    "\n",
    "#will want to change the fire date break point to be later..\n",
    "precip_series = []\n",
    "temp_series = []\n",
    "ndvi_before_series = []\n",
    "ndvi_after_series = []\n",
    "lai_before_series = []\n",
    "lai_after_series = []\n",
    "vcf_tree_before_series = []\n",
    "vcf_tree_after_series = []\n",
    "event_series = []\n",
    "\n",
    "# Parsing the precip data\n",
    "for i, row in precip_gdf.iterrows():\n",
    "    #i is the row index; row is the row value container\n",
    "    # print(i)\n",
    "    # print('\\n\\n\\n',row)\n",
    "    # break\n",
    "    # Get the dates from the column name\n",
    "    dates = [parser.parse(j[6:]) for j in row.index[:-3]]\n",
    "    dates = [d.replace(day=calendar.monthrange(d.year, d.month)[1], hour=23, minute = 59) for d in dates] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "\n",
    "    # Get the fire date\n",
    "        #increment fire date by 1 month\n",
    "\n",
    "    fire_date = row.Ig_Date #+ relativedelta(months=+1)\n",
    "    split_date = fire_date + relativedelta(months = postfire_mo)\n",
    "\n",
    "    # Get the date of 10 years before the fire (or split date)\n",
    "    #increment fire date by n months\n",
    "    fire_date_10_before = split_date - relativedelta(months = prefire_mo )#+ postfire_mo)\n",
    "\n",
    "    # Put the data into a pandas time series, clip it to the temporal extent, and append to a list\n",
    "    precip_series.append(pd.Series(row.values[:-3], index=dates, name=i)[fire_date_10_before:split_date].tolist())\n",
    "    event_series.append(row.values[-2]) #event ID value\n",
    "\n",
    "\n",
    "# Parsing the temperature data\n",
    "for i, row in temp_gdf.iterrows():\n",
    "    # Get the dates from the column name\n",
    "    dates = [parser.parse(j[4:]) for j in row.index[:-2]]\n",
    "    dates = [d.replace(day=calendar.monthrange(d.year, d.month)[1], hour=23, minute = 59) for d in dates] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "\n",
    "    # Get the fire date\n",
    "    fire_date = row.Ig_Date #+ relativedelta(months=+1)\n",
    "    split_date = fire_date + relativedelta(months = postfire_mo)\n",
    "    # Get the date of 10 years before the fire\n",
    "    fire_date_10_before = split_date - relativedelta(months = prefire_mo )#+ postfire_mo)\n",
    "    #increment fire date by 1 month\n",
    "    # fire_date = fire_date + relativedelta(months=+1)\n",
    "    # Put the data into a pandas time series, clip it to the temporal extent, and append to a list\n",
    "    temp_series.append(pd.Series(row.values[:-2], index=dates, name= i)[fire_date_10_before:split_date].tolist())\n",
    "\n",
    "# Parsing the NDVI data\n",
    "for i, row in ndvi_gdf.iterrows():\n",
    "    # Get the dates from the column name\n",
    "    dates = [parser.parse(j[4:]) for j in row.index[:-2]] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "    dates = [d.replace(day=calendar.monthrange(d.year, d.month)[1], hour=23, minute = 59) for d in dates] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "\n",
    "    # Get the fire date\n",
    "    fire_date = row.Ig_Date #+ relativedelta(months=+1)\n",
    "    split_date = fire_date + relativedelta(months = postfire_mo)\n",
    "    # Get the date of 10 years before the fire\n",
    "    fire_date_10_before = split_date - relativedelta(months = prefire_mo )#+ postfire_mo)\n",
    "    # Get the date of 9 years after the fire (not 10 due to a few fires that this goes to far into the future for)\n",
    "    fire_date_10_after = split_date + relativedelta(months = 110-postfire_mo)\n",
    "    # Adding both datasets to seperate lists\n",
    "    ndvi_before_series.append(pd.Series(row.values[:-2], index=dates, name= i)[fire_date_10_before:split_date].tolist()) #exclusive of fire_date\n",
    "    ndvi_after_series.append(pd.Series(row.values[:-2], index=dates, name= i)[split_date:fire_date_10_after].tolist()) #inclusive of fire_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_agdNOk1WnJc"
   },
   "outputs": [],
   "source": [
    "#parse LAI data\n",
    "geom_series =[ ]\n",
    "\n",
    "for i, row in lai_gdf.iterrows():\n",
    "  \n",
    "    # Get the dates from the column name\n",
    "    dates = [parser.parse(j[3:6]+'20'+j[-2:]) for j in row.index[1:-1]] #only using row index [1:-1] to exclude first and last col, which are ig_date and geometry,respectively\n",
    "    dates = [d.replace(day=calendar.monthrange(d.year, d.month)[1], hour=23, minute = 59) for d in dates] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "\n",
    "    # Get the fire date\n",
    "    fire_date = row.Ig_Date #+ relativedelta(months=+1)\n",
    "    split_date = fire_date + relativedelta(months = postfire_mo)\n",
    "    # Get the date of 10 years before the fire\n",
    "    fire_date_10_before = split_date - relativedelta(months = prefire_mo )#+ postfire_mo)\n",
    "    # Get the date of 9 years after the fire (not 10 due to a few fires that this goes to far into the future for)\n",
    "    fire_date_10_after = split_date + relativedelta(months = 110-postfire_mo)\n",
    "    # Adding both datasets to seperate lists\n",
    "    lai_before_series.append(pd.Series(row.values[1:-1], index=dates, name= i)[fire_date_10_before:split_date].tolist()) #exclusive of fire_date\n",
    "    lai_after_series.append(pd.Series(row.values[1:-1], index=dates, name= i)[split_date:fire_date_10_after].tolist()) #inclusive of fire_date\n",
    "    geom_series.append(row.geometry)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lai_before_series[96])\n",
    "plt.plot(lai_after_series[96])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9zHgAbhYM6G"
   },
   "outputs": [],
   "source": [
    "#parse VCF Tree data\n",
    "\n",
    "for i, row in vcf_tree_gdf.iterrows():\n",
    "    # Get the dates from the column name\n",
    "    dates = [parser.parse(j[4:7]+'20'+j[-2:]) for j in row.index[1:-1]] #only using row index [1:-1] to exclude first and last col, which are ig_date and geometry,respectively\n",
    "    dates = [d.replace(day=calendar.monthrange(d.year, d.month)[1], hour=23, minute = 59) for d in dates] #format: NDVIFeb2000 - wer're selecting Feb2000\n",
    "\n",
    "    # print(row.index[1][4:7]+'20'+row.index[1][-2:])\n",
    "    # break\n",
    "    # Get the fire date\n",
    "    fire_date = row.Ig_Date #+ relativedelta(months=+1)\n",
    "    split_date = fire_date + relativedelta(months = postfire_mo)\n",
    "    # Get the date of 10 years before the fire\n",
    "    fire_date_10_before = split_date - relativedelta(months = prefire_mo )#+ postfire_mo)\n",
    "    # Get the date of 9 years after the fire (not 10 due to a few fires that this goes to far into the future for)\n",
    "    fire_date_10_after = split_date + relativedelta(months = 110-postfire_mo)\n",
    "    # Adding both datasets to seperate lists\n",
    "    vcf_tree_before_series.append(pd.Series(row.values[1:-1], index=dates, name= i)[fire_date_10_before:split_date].tolist()) #exclusive of fire_date\n",
    "    vcf_tree_after_series.append(pd.Series(row.values[1:-1], index=dates, name= i)[split_date:fire_date_10_after].tolist()) #inclusive of fire_date\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b8oy899MD7j"
   },
   "outputs": [],
   "source": [
    "\n",
    "# reorganize = pd.DataFrame(list(zip(*[ lai_before_series,event_series])),\n",
    "#                           columns = ['lai','Event_ID'])\n",
    "# print(reorganize.head())\n",
    "# reorganize_gdf = gpd.GeoDataFrame(reorganize,\n",
    "#                                   geometry = geom_series,\n",
    "#                                   crs = time_series_fire_date.crs)\n",
    "\n",
    "\n",
    "# time_series_fire_date.head()\n",
    "# #this is fine too\n",
    "# save_this = reorganize_gdf[['Event_ID','geometry']].copy()\n",
    "# save_this.to_file(\"next_test56.shp\",geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xrx_zLNeB4_R",
    "outputId": "22644493-ac93-4ef1-d83a-d87870435cd4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#check that theyre all the same length\n",
    "print(len(ndvi_after_series), len(ndvi_after_series[1]))#[1].shape)\n",
    "print(len(ndvi_before_series), len(ndvi_before_series[1]))#[1].shape)\n",
    "print(len(lai_after_series), len(lai_after_series[1]))#[1].shape)\n",
    "\n",
    "\n",
    "for i in range(len(ndvi_before_series)):\n",
    "  if len(precip_series[i]) !=prefire_mo or len(temp_series[i]) !=prefire_mo or len(ndvi_before_series[i])!=prefire_mo or len(lai_before_series[i])!= prefire_mo or len(vcf_tree_before_series[i]) !=prefire_mo:\n",
    "    print(i)\n",
    "    raise\n",
    "  if np.isnan(precip_series[i][-1]) and not np.isnan(precip_series[i][1]):\n",
    "    print(\"NAN\", precip_series[i][-1])\n",
    "    raise\n",
    "\n",
    "for i in range(len(ndvi_after_series)):\n",
    "  if len(ndvi_after_series[i])!=110-postfire_mo or len(lai_after_series[i])!=110-postfire_mo or (np.isnan(ndvi_after_series[i][-1]) and not np.isnan(ndvi_after_series[i][1])):\n",
    "    print(i, len(ndvi_after_series[i]), ndvi_after_series[i][-1])\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhLS-nTLOD28"
   },
   "source": [
    "### Re-create gdf where columns contain listed timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3sABahpJHj-"
   },
   "outputs": [],
   "source": [
    "# Reingests all of the data into a geodataframe that has the time series for\n",
    "#each variable stored in each cell, and the ndvi split up before and after fire date\n",
    "\n",
    "Ig_Series = time_series_fire_date['Ig_Date']\n",
    "Id_Series = time_series_fire_date['Event_ID']#event_series (also works) #time_series_fire_date['Event_ID'] #event series is list of fires for each row in df (so repeating fire ids)\n",
    "#Note on Event_ID [from GEE]: Unique identifier for each event (21 characters). Calculated from source data (ICS209, FedFire, etc.) ;\n",
    "#   each time an event is created or updated using the state, lat/long coordinates (ig_lat, ig_long). Note that for longitudes less than 100Â° a leading zero is added to maintain 21 characters.\n",
    "\n",
    "reorganize = pd.DataFrame(list(zip(*[precip_series, temp_series, ndvi_before_series, lai_before_series, vcf_tree_before_series,\n",
    "                                      ndvi_after_series, lai_after_series,vcf_tree_after_series,\n",
    "                                      Ig_Series, Id_Series, time_series_fire_date.geometry])),\n",
    "                          columns = ['precip', 'temp', 'ndvi_before', 'lai_before', 'vcf_tree_before',\n",
    "                                     'ndvi_after', 'lai_after', 'vcf_tree_after',\n",
    "                                     'Ig_Date','Event_ID', 'geom'])\n",
    "reorganize_gdf = gpd.GeoDataFrame(reorganize,\n",
    "                                  geometry = 'geom',#time_series_fire_date.geometry,\n",
    "                                  crs = time_series_fire_date.crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_cols = [i for i in reorganize_gdf.columns if 'lai_b' in i]\n",
    "sdfg=reorganize_gdf[temp_cols].iloc[96].tolist()\n",
    "plt.plot(sdfg[0])\n",
    "\n",
    "temp_cols = [i for i in reorganize_gdf.columns if 'lai_a' in i]\n",
    "sdfg=reorganize_gdf[temp_cols].iloc[96].tolist()\n",
    "plt.plot(sdfg[0])\n",
    "#.plot(label='LAI',color='green')\n",
    "# plt.legend(loc='upper right')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_ey_4ZNOPIu"
   },
   "source": [
    "### Read static-var rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-xSAck-fy39"
   },
   "outputs": [],
   "source": [
    "# Loop through all static rasters and extract the point data needed for them\n",
    "# There are 4 rasters that cover different spatial extents of Colorado. The full state was too big to export as one raster\n",
    "static_rasters =[static_fp1, static_fp2, static_fp3, static_fp4]\n",
    "#NOTE******** this static stack contains the wrong version of burn severity \n",
    "static_gdfs = []\n",
    "for raster in static_rasters:\n",
    "    print(raster, 'is processing..')\n",
    "    print(os.path.exists(raster))\n",
    "    static_gdfs.append(turn_raster_output_to_gdf_STATIC(raster, time_series_fire_date, ['slope', 'chili', 'elevation', 'aspect', 'severity', 'mtpi'],prefire_mo))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add burn severity seperately from static rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly add in burn severity bc this layer has a resolution-based issue and thus leads to no data\n",
    "\n",
    "# Loop through all static rasters and extract the point data needed for them\n",
    "# There are 4 rasters that cover different spatial extents of Colorado. The full state was too big to export as one raster\n",
    "burnsev_rasters_prj = [os.path.join(burn_main_dir,tif) for tif in os.listdir(burn_main_dir) if tif.endswith('.tif')]\n",
    "burnsev_rasters_withPoints = [] #store which rasters actually do have points in them (for burn sev this should be all of them)\n",
    "burnsev_gdfs = []\n",
    "\n",
    "for raster in burnsev_rasters_prj:\n",
    "    print(raster, 'is processing..')\n",
    "    to_add = turn_raster_output_to_gdf_STATIC(raster, time_series_fire_date, ['temp_sev'],prefire_mo)\n",
    "    if to_add is not None:\n",
    "        burnsev_rasters_withPoints.append(raster)\n",
    "    burnsev_gdfs.append(to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqOsPjxdOY2K"
   },
   "source": [
    "### Add static variables to final gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "id": "Xou1DFKPS8wS",
    "outputId": "3f5f99ad-249b-4c6c-a7bb-57025cd6e428"
   },
   "outputs": [],
   "source": [
    "# Append all geodataframes from static variables into one geodataframe\n",
    "static_gdf = static_gdfs[0].append(static_gdfs[1]).append(static_gdfs[2]).append(static_gdfs[3]) #instead of append, may need to use pd.concat in future\n",
    "\n",
    "print('static_gdf',static_gdf.shape)\n",
    "static_gdf = pd.concat(static_gdfs[g] for g in range(len(static_gdfs)))\n",
    "burnsev_gdf = pd.concat(burnsev_gdfs[g] for g in range(len(burnsev_gdfs)))\n",
    "\n",
    "# Run a spatial join to combine the time series variables and static variables by point\n",
    "all_gdf = gpd.sjoin(static_gdf, reorganize_gdf)#,how='right')\n",
    "print('allgdf init shp:',all_gdf.shape)\n",
    "all_gdf = all_gdf.rename(columns={'index_left': 'index_lef1', 'index_right':'index_right1'})\n",
    "\n",
    "#Drop duplicate points (based on geometry)\n",
    "all_gdf.drop_duplicates(subset='geometry',keep='first',inplace=True)\n",
    "print(all_gdf.shape,'is shp after drop duplicates (based on geometry field)')\n",
    "\n",
    "all_gdf = gpd.sjoin(all_gdf, burnsev_gdf) # need ot add all form bursev_gdfs list\n",
    "print('all gdf w burn severity shape:',all_gdf.shape)\n",
    "\n",
    "all_gdf['severity'] = all_gdf['temp_sev'] #assign original severity column with true severity values \n",
    "all_gdf = all_gdf.drop('temp_sev', axis=1) #drop the true severity values stored in temporary (now duplicate) column\n",
    "print(all_gdf.shape,'is shape w severity before dropping duplicate geoms')\n",
    "all_gdf=all_gdf.drop_duplicates(subset='geometry',keep='first')\n",
    "\n",
    "print(all_gdf.shape,'is shp after drop duplicates (based on geometry field)')\n",
    "all_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup=all_gdf['geometry'].duplicated()\n",
    "# mask = all_gdf['severity'][dup] #(1135,)\n",
    "# # all_gdf[dup]\n",
    "\n",
    "\n",
    "# print(mask.shape)\n",
    "\n",
    "# mask.index\n",
    "\n",
    "# for i in mask.index:#range(120):\n",
    "# #     print(mask[i])\n",
    "# #     print(all_gdf['severity'][i])\n",
    "#     print(all_gdf.loc[all_gdf.index[i], 'severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wbo8_ydkeih2"
   },
   "outputs": [],
   "source": [
    "band_names = []\n",
    "years = list(range(2000, 2022))\n",
    "months = list(calendar.month_name[1:])\n",
    "vars = ['precip', 'temp', 'NDVI', 'LAI', 'VCF_tree']\n",
    "\n",
    "for year in years:\n",
    "  for month in months:\n",
    "    for var in vars:\n",
    "      band_names.append(var + month[:3] + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows where there is no data in the static variable columns (17 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gdf_cp =  all_gdf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "dropcnt=0\n",
    "# Function to calculate nanmean of a list and handle None case\n",
    "def safe_nanmean(lst):\n",
    "    return np.nanmean(lst) if not all(np.isnan(lst)) else None\n",
    "\n",
    "store_inx=[]\n",
    "# Iterate through rows in the GeoDataFrame\n",
    "for index, row in all_gdf_cp.iterrows():\n",
    "\n",
    "    # Calculate nanmean of the 'slope' column for each row\n",
    "    nanmean_value = safe_nanmean(row['slope'])\n",
    "    \n",
    "    # Check if nanmean is None (all values were nan)\n",
    "    if nanmean_value is None:\n",
    "        dropcnt+=1\n",
    "        # Drop the row if nanmean is None\n",
    "        all_gdf_cp.drop(index, inplace=True)\n",
    "\n",
    "# Now 'gdf' will not contain rows where nanmean of 'slope' is None\n",
    "dropcnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_gdf.shape)\n",
    "print(all_gdf_cp.shape) # should be 998,19 ( bc 17 less than 1015)\n",
    "\n",
    "all_gdf = all_gdf_cp.copy()\n",
    "# all_gdf_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI-ECGibRgxm"
   },
   "source": [
    "### Export .GDF\n",
    "(export to shapefiles - first the full/raw data (first element though) and in the next cell export the variable averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Export all_gdf \n",
    "all_gdf.to_pickle(\"all_gdf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a list and returns its first element\n",
    "def list_zero(x):\n",
    "    if type(x)==list:\n",
    "        return x[0]\n",
    "#     else:\n",
    "#         print('none','is x')\n",
    "\n",
    "def create_str(in_list):\n",
    "    return \"_\".join([str(k) for k in in_list])\n",
    "\n",
    "static_single = all_gdf.copy()\n",
    "dyn_vars = ['precip','temp', 'ndvi_before', 'lai_before', 'vcf_tree_before','ndvi_after', 'lai_after', 'vcf_tree_after']\n",
    "static_vars = ['slope', 'chili', 'elevation', 'aspect', 'severity', 'mtpi']\n",
    "\n",
    "\n",
    "# For each static variable, map it to its first element\n",
    "for var in static_vars:\n",
    "    print(var)\n",
    "    static_single[var] = static_single[var].map(list_zero)\n",
    "\n",
    "for var in dyn_vars:\n",
    "    print(var)\n",
    "    static_single[var] = static_single[var].map(create_str)\n",
    "    \n",
    "#     print(static_single[var])\n",
    "print('done')\n",
    "# Perform a spatial join between static_single and time_series_fire_date, and create new columns for Year, Month, and Day based on Ig_Date\n",
    "new_join = static_single#gpd.sjoin(static_single, time_series_fire_date)\n",
    "new_join['Year'] = new_join['Ig_Date'].map(lambda x: x.year)\n",
    "new_join['Month'] = new_join['Ig_Date'].map(lambda x: x.month)\n",
    "new_join['Day'] = new_join['Ig_Date'].map(lambda x: x.day)\n",
    "\n",
    "# Drop the Ig_Date column from new_join\n",
    "new_join = new_join.drop('Ig_Date', axis=1)\n",
    "\n",
    "fixed_band_names = {}\n",
    "\n",
    "# Rename bands to be smaller names so it's shapefile friendly\n",
    "for i in band_names:\n",
    "  if i.startswith('N'):\n",
    "    fixed_band_names[i] = i.replace('NDVI', 'N')\n",
    "  elif i.startswith('t'):\n",
    "    fixed_band_names[i] = i.replace('temp', 'T')\n",
    "  elif i.startswith('p'):\n",
    "    fixed_band_names[i] = i.replace('precip', 'P')\n",
    "  elif i.startswith('L'):\n",
    "    fixed_band_names[i] = i.replace('LAI', 'L')\n",
    "  elif i.startswith('V'):\n",
    "    fixed_band_names[i] = i.replace('VCF_tree', 'F') #replace w F for forest since T is used\n",
    "  else:\n",
    "    print(i)\n",
    "\n",
    "new_join = new_join.rename(columns=fixed_band_names)\n",
    "\n",
    "# Write to a shapefile\n",
    "print(\"writing new shapefile: Quant_Fire_Data_list\")\n",
    "\n",
    "#uncomment this to create shapefile - this takes a while to run\n",
    "new_join.to_file(\"Quant_Fire_Data_list.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlpath='LSTM_Data.csv'\n",
    "\n",
    "all_gdf_toexcel = all_gdf.rename(columns={'ndvi_before': 'pre_ndvi', \n",
    "                                          'lai_before':'pre_lai', \n",
    "                                          'ndvi_after': 'post_ndvi', \n",
    "                                          'lai_after':'post_lai',\n",
    "                                          'vcf_tree_before':'pre_tree',\n",
    "                                         'vcf_tree_after':'post_tree'})\n",
    "\n",
    "\n",
    "all_gdf_toexcel['x'] = all_gdf_toexcel['geometry'].x\n",
    "all_gdf_toexcel['y'] = all_gdf_toexcel['geometry'].y\n",
    "\n",
    "# all_gdf_toexcel['ig_month'] = all_gdf_toexcel['Ig_Date'].month\n",
    "all_gdf_toexcel['ig_month'] = [this_day.month for this_day in all_gdf_toexcel['Ig_Date']]\n",
    "all_gdf_toexcel['ig_day'] = [this_day.day for this_day in all_gdf_toexcel['Ig_Date']]\n",
    "all_gdf_toexcel['ig_year'] = [this_day.year for this_day in all_gdf_toexcel['Ig_Date']]\n",
    "\n",
    "all_gdf_toexcel = all_gdf_toexcel.drop(['index_right1','index_right'], axis=1)\n",
    "\n",
    "# all_gdf_toexcel= pd.DataFrame(all_gdf_toexcel)\n",
    "\n",
    "# print(all_gdf_toexcel.head()[:1])\n",
    "\n",
    "# all_gdf_toexcel.to_file(xlpath)#, driver='excel')\n",
    "all_gdf_toexcel.to_csv(xlpath)#, index=0)\n",
    "# pd.__version__ #'1.5.3'\n",
    "\n",
    "#read back in:\n",
    "\n",
    "#instead of going through all the data preprocessing above - just use this excel sheet\n",
    "new_gdfx = pd.read_csv(xlpath)\n",
    "new_gdfx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AC15qHCh48v_",
    "outputId": "e75ae8ad-4d9f-493b-b82d-187344221ce1"
   },
   "outputs": [],
   "source": [
    "#NOTE: 17 points are outside of the static variable raster so have no data there\n",
    "\n",
    "fire_list ={event_id:{} for event_id in set(all_gdf['Event_ID'])}\n",
    "\n",
    "\n",
    "list_variables = ['slope', 'chili', 'elevation', 'aspect', 'severity', 'mtpi',\n",
    "                    'precip', 'temp', 'ndvi_before','lai_before','vcf_tree_before',\n",
    "                   'ndvi_after','lai_after', 'vcf_tree_after']#,'geometry']\n",
    "\n",
    "count_errors = 0\n",
    "list_error_fires=[]\n",
    "#iterate through rows of gdf\n",
    "for index, row in all_gdf.iterrows():\n",
    "    event_id = row[\"Event_ID\"]\n",
    "    for variable in list_variables:\n",
    "        if variable in fire_list[event_id].keys():\n",
    "            try:\n",
    "                \n",
    "                fire_list[event_id][variable].extend(row[variable]) #here\n",
    "            except:\n",
    "                count_errors+=1\n",
    "                list_error_fires.append(event_id)\n",
    "                \n",
    "        else:\n",
    "            fire_list[event_id][variable] = list(row[variable]) \n",
    "            \n",
    "print('num errors',count_errors/4) # divide by 4 bc thats how many layers are in static bands (minus burn severity which was on its own)\n",
    "print('error ids:', set(list_error_fires))\n",
    "\n",
    "#want dataframe with rows = event id and columns = \"slope_mean\", \"aspect_std\", etc.\n",
    "fire_stats = pd.DataFrame()\n",
    "# Define the data as a dictionary\n",
    "# data = {'mean': [1, 2, 3], 'std': [0.1, 0.2, 0.3]}\n",
    "\n",
    "# Create the DataFrame\n",
    "rownames = [i+'_'+j for i in ['mean','std','min','max'] for j in list_variables]\n",
    "empty_array=np.zeros((len(fire_list),len(rownames)))\n",
    "df = pd.DataFrame(empty_array, index=fire_list, columns=rownames)\n",
    "\n",
    "this_idx=-1\n",
    "for event_id, variable in fire_list.items():\n",
    "    print(event_id,\"is fire\")\n",
    "    #iterate through fires\n",
    "    #event id is ID of fire e.g. CO3872810860220100712\n",
    "    #variable is a dictionary, where keys are like 'slope','mtpi' and values are list of list of values\n",
    "    for feat,stat_list in variable.items():\n",
    "        #feat is something like 'slope', 'mtpi'\n",
    "        this_mean = round(np.nanmean(stat_list),3)\n",
    "        this_std = round(np.nanstd(stat_list),3)\n",
    "        this_min = round(np.min(stat_list),3)\n",
    "        this_max = round(np.max(stat_list),3)\n",
    "        df.loc[[event_id], ['mean_{}'.format(feat)]] = this_mean\n",
    "        df.loc[[event_id], ['std_{}'.format(feat)]] = this_std\n",
    "        df.loc[[event_id], ['min_{}'.format(feat)]] = this_min\n",
    "        df.loc[[event_id], ['max_{}'.format(feat)]] = this_max\n",
    "\n",
    "df.to_csv(\"per_fire_summary_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhWcG-B-OhOD"
   },
   "source": [
    "## Split training and test fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3ITyZXG4YOo",
    "outputId": "eeacfedd-4e62-45c1-e19b-f6d9436a8597"
   },
   "outputs": [],
   "source": [
    "# Run a custom test/train split so that test is made of completely seperate fires then train\n",
    "test = []\n",
    "train = []\n",
    "train_id_dict = {}\n",
    "test_id_dict = {}\n",
    "total_points, test_fire_num, train_fire_num = 0, 0, 0\n",
    "# Group by date loops through the dataframe by fire (via ignition date)\n",
    "for group in all_gdf.groupby('Event_ID'):\n",
    "  # Check how many points are already in the test dataset. Setting this to 700 results in a roughly 80/20 split\n",
    "  if total_points < 790:\n",
    "    print('train',len(group[1]))\n",
    "    range_g = str(total_points) + '-' + str(total_points+len(group[1])-1)\n",
    "    total_points += len(group[1])\n",
    "    train.append(group[1][['slope', 'chili', 'elevation', 'aspect', 'severity',\n",
    "                           'mtpi', 'precip', 'temp', 'ndvi_before','lai_before', 'vcf_tree_before',\n",
    "                           'ndvi_after','lai_after','vcf_tree_after','geometry', 'Event_ID']])\n",
    "    train_id_dict[group[1]['Event_ID'].values[0]] = [len(group[1]), range_g]\n",
    "    train_fire_num += 1\n",
    "\n",
    "  else:\n",
    "    print(len(group[1]))\n",
    "    range_g = str(total_points) + '-' + str(total_points+len(group[1])-1)\n",
    "    total_points += len(group[1])\n",
    "    test.append(group[1][['slope', 'chili', 'elevation', 'aspect', 'severity',\n",
    "                          'mtpi', 'precip', 'temp', 'ndvi_before','lai_before','vcf_tree_before',\n",
    "                          'ndvi_after','lai_after','vcf_tree_after','geometry','Event_ID']])\n",
    "    test_fire_num += 1\n",
    "    test_id_dict[group[1]['Event_ID'].values[0]] = [len(group[1]), range_g]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEaype__zK17",
    "outputId": "898d0f20-afe7-4812-f062-a719ed104a92"
   },
   "outputs": [],
   "source": [
    "# code to randomly select 6 fires for test and 23 for train\n",
    "test = []\n",
    "train = []\n",
    "validation = []\n",
    "\n",
    "id_index = list(all_gdf.columns).index('Event_ID') # get index value with which to get event_id from each row (should be 17)\n",
    "\n",
    "total_points, test_fire_num, train_fire_num = 0, 0, 0\n",
    "\n",
    "all_ids = list(set(train_id_dict.keys())) + list(set(test_id_dict.keys()))\n",
    "\n",
    "\n",
    "# train_id, test_id, t0, t1 = train_test_split(all_ids, all_ids, test_size = .15, random_state = 3)\n",
    "# train_id, val_id, t0, t1 = train_test_split(train_id, train_id, test_size = .15, random_state = 3)\n",
    "# del t0,t1\n",
    "\n",
    "# #setting this ***manually*** b/c each time I reconnected colab runtime even with same random_state, split was not reproducable\n",
    "train_id = ['NM3692010445620110612','CO3817710507320121023', 'CO3970610755420100628', 'CO3843610898920120525', 'CO3872810860220100712', 'CO4036110556320121009', 'UT3967210907720100826', 'CO3888410493320120623', 'CO3740210724320120513', 'CO3825210820820110803', 'UT3977510922020120629', 'CO4058910540420120609', 'CO3812310818320100522', 'CO4054610858420100720', 'CO4053710538120110401', 'CO3823310568320110612', 'CO3976210526320110320', 'CO3925110844620110807', 'CO3709810719920101012', 'CO3781310546020100606']\n",
    "val_id = ['CO3726810830320120622', 'CO3935510767920100507', 'CO4005110538520100906', 'CO3943610521720120326']\n",
    "test_id = ['CO3747210346920110607', 'NM3700010423620110526', 'NM3696310515520100523', 'CO3741010757920121016', 'CO3894510543620120617'] #removed: 'CO3740210724320120513'\n",
    "#NOTE: swapped little sand fire into test and track fire into train! 11-22-23\n",
    "\n",
    "print(\"LENs:\",len(train_id), len(val_id), len(test_id))\n",
    "\n",
    "print(train_id)\n",
    "print(val_id)\n",
    "print(test_id)\n",
    "\n",
    "print(\"# of Train Fires:\", len(train_id))\n",
    "print(\"# of Validation Fires:\", len(val_id))\n",
    "print('# of Test Fires:', len(test_id))\n",
    "\n",
    "#create dict of lists where each sublist contains all points in n test fire\n",
    "testfire_dict = {}\n",
    "\n",
    "for row in all_gdf.iterrows():\n",
    "\n",
    "  if row[1][id_index] in train_id:\n",
    "\n",
    "      train.append(row[1][['slope', 'chili', 'elevation', 'aspect', 'severity', 'geometry','Event_ID',\n",
    "                             'mtpi', 'precip', 'temp', 'ndvi_before','lai_before','vcf_tree_before',\n",
    "                            'ndvi_after','lai_after', 'vcf_tree_after']])\n",
    "  elif row[1][id_index] in val_id:\n",
    "      validation.append(row[1][['slope', 'chili', 'elevation', 'aspect', 'severity', 'geometry','Event_ID',\n",
    "                             'mtpi', 'precip', 'temp', 'ndvi_before', 'lai_before', 'vcf_tree_before',\n",
    "                                'ndvi_after','lai_after','vcf_tree_after']])\n",
    "  elif row[1][id_index] in test_id:\n",
    "      test.append(row[1][['slope', 'chili', 'elevation', 'aspect', 'severity', 'geometry','Event_ID',\n",
    "                          'mtpi', 'precip', 'temp', 'ndvi_before', 'lai_before','vcf_tree_before',\n",
    "                          'ndvi_after','lai_after','vcf_tree_after']])\n",
    "      if row[1][id_index] in testfire_dict.keys():\n",
    "        testfire_dict[row[1][id_index]].append(row[1][['slope', 'chili', 'elevation', 'aspect', 'severity',\n",
    "                          'mtpi', 'precip', 'temp', 'ndvi_before', 'lai_before','vcf_tree_before',\n",
    "                                                 'ndvi_after','lai_after','vcf_tree_after']])\n",
    "      else:\n",
    "        testfire_dict[row[1][id_index]] = [row[1][['slope', 'chili', 'elevation', 'aspect', 'severity',\n",
    "                          'mtpi', 'precip', 'temp', 'ndvi_before', 'lai_before','vcf_tree_before',\n",
    "                                             'ndvi_after','lai_after','vcf_tree_after']]]\n",
    "  else:\n",
    "    print(\"ERROR: point not assigned to set:\",row[1][id_index])\n",
    "\n",
    "print(\"# points in training dataset:\", len(train))\n",
    "print(\"# points in validation dataset:\", len(validation))\n",
    "\n",
    "print(\"# points in test dataset:\", len(test))\n",
    "print(\"Train/Val/Test Split:\", len(train)/(len(test)+len(train)+len(validation)), '/',\\\n",
    "      len(validation)/(len(test)+len(train)+len(validation)),'/',len(test)/(len(test)+len(train)+len(validation)))\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "train_df = pd.DataFrame(train)\n",
    "val_df = pd.DataFrame(validation)\n",
    "\n",
    "testfire_df = {}\n",
    "for key,val in testfire_dict.items():\n",
    "  new_val = pd.DataFrame(val)\n",
    "  testfire_df[key] = new_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_cols = [i for i in test_df.columns if 'lai_a' in i]\n",
    "plt.plot(test_df[temp_cols].iloc[96].tolist()[0])\n",
    "\n",
    "temp_cols = [i for i in test_df.columns if 'lai_b' in i]\n",
    "plt.plot(test_df[temp_cols].iloc[96].tolist()[0])\n",
    "# .plot(label='LAI',color='green')\n",
    "# plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "H8xGT19ZOuBF",
    "outputId": "1ebf2323-10f9-4e49-e93b-4299a167119b"
   },
   "outputs": [],
   "source": [
    "# create table that has each fire, # of points in the fire bound, and whether\n",
    "# fire is designated as training or test\n",
    "#and at the bottom, the total # of training and test points\n",
    "\n",
    "#These parameters must be 2D lists, in which the outer lists define the rows and\n",
    "#the inner list define the column values per row.\n",
    "train_lab = [\"Train\" for i in range(len(train_id_dict))]\n",
    "test_lab = [\"Test\" for i in range(len(test_id_dict))]\n",
    "\n",
    "# range_pt_ids =[i[1] for i in list(train_id.values)]\n",
    "num_pts_list = [i[0] for i in list(train_id_dict.values())]+[i[0] for i in list(test_id_dict.values())]\n",
    "range_pts_list = [i[1] for i in list(train_id_dict.values())]+[i[1] for i in list(test_id_dict.values())]\n",
    "print(len(range_pts_list),len(num_pts_list))\n",
    "print(sum(num_pts_list),\"is total number of points\")\n",
    "print(\"Train fires:\",list(train_id_dict.keys()))\n",
    "print(\"Test fires:\",list(test_id_dict.keys()))\n",
    "\n",
    "\n",
    "table_list =  pd.DataFrame({\"Set\":train_lab+test_lab,\n",
    "                            \"MTBS Fire ID\": list(train_id_dict.keys())+list(test_id_dict.keys()),\n",
    "                            \"Num. Points (pre null filter)\": num_pts_list\n",
    "                            # \"Range Point IDs\":range_pts_list\n",
    "                            })\n",
    "table_list\n",
    "# col_labels = [\"MTBS Fire ID\",\"# Points\",\"Set (Train or Test)\"]\n",
    "# row_labels = [\"\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = ['NM3692010445620110612','CO3817710507320121023', 'CO3970610755420100628', 'CO3843610898920120525', 'CO3872810860220100712', 'CO4036110556320121009', 'UT3967210907720100826', 'CO3888410493320120623', 'CO3740210724320120513', 'CO3825210820820110803', 'UT3977510922020120629', 'CO4058910540420120609', 'CO3812310818320100522', 'CO4054610858420100720', 'CO4053710538120110401', 'CO3823310568320110612', 'CO3976210526320110320', 'CO3925110844620110807', 'CO3709810719920101012', 'CO3781310546020100606']\n",
    "val_id = ['CO3726810830320120622', 'CO3935510767920100507', 'CO4005110538520100906', 'CO3943610521720120326']\n",
    "test_id = ['CO3747210346920110607', 'NM3700010423620110526', 'NM3696310515520100523', 'CO3741010757920121016', 'CO3894510543620120617'] #removed: 'CO3740210724320120513'\n",
    "#NOTE: swapped little sand fire into test and track fire into train! 11-22-23\n",
    "\n",
    "for i in val_id:\n",
    "    if i in test_id or i in train_id:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATNLuwBbPOrf"
   },
   "source": [
    "## Viz train vs. test fire bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9kBdMzbMn40"
   },
   "outputs": [],
   "source": [
    "\n",
    "#note that there are 29 IDs here vs. 28 fires that data are split on bc one fire crosses the CO/UT border\n",
    "#and thus has 2 dif Event_ID values for it\n",
    "\n",
    "#seperate fires from fire_bounds in training set and test set\n",
    "\n",
    "list_train_id = list(set(train_id))\n",
    "list_val_id = list(set(val_id))\n",
    "list_test_id = list(set(test_id))\n",
    "\n",
    "mask = fire_bounds_dates['Event_ID'].isin(list_train_id)\n",
    "mask_val = fire_bounds_dates['Event_ID'].isin(list_val_id)\n",
    "mask_test = fire_bounds_dates['Event_ID'].isin(list_test_id)\n",
    "\n",
    "train_fire_bounds = fire_bounds_dates[mask]\n",
    "val_fire_bounds = fire_bounds_dates[mask_val]\n",
    "test_fire_bounds = fire_bounds_dates[mask_test]\n",
    "\n",
    "\n",
    "print(len(train_fire_bounds), len(val_fire_bounds), len(test_fire_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "wT4nkfUiPOF1",
    "outputId": "325433a0-04a6-4f85-baee-e8904a6783ba"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "train_fire_bounds.boundary.plot(ax=ax, color='blue', linewidth=1, label='Train')\n",
    "val_fire_bounds.boundary.plot(ax=ax, color='green', linewidth=1, label='Validation')\n",
    "test_fire_bounds.boundary.plot(ax=ax, color='orange', linewidth=1, label='Test')\n",
    "plt.title(\"Fire Boundaries\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"Training_vs_Test_Fire_Bounds\",dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST64nU92xdsM"
   },
   "source": [
    "## Fill NaN values in NDVI_Before series in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J27OKX4RkX0v"
   },
   "outputs": [],
   "source": [
    "# forward-fill NaN values in ndvi_before in dataframes\n",
    "\n",
    "for list_i in range(len(test_df.ndvi_before.values)):\n",
    "  df_temp = pd.Series(test_df.ndvi_before.values[list_i])\n",
    "  new_l = df_temp.fillna(method='ffill')\n",
    "  test_df.ndvi_before.values[list_i] = list(new_l)\n",
    "\n",
    "for list_i in range(len(train_df.ndvi_before.values)):\n",
    "  df_temp = pd.Series(train_df.ndvi_before.values[list_i])\n",
    "  new_l = df_temp.fillna(method='ffill')\n",
    "  train_df.ndvi_before.values[list_i] = list(new_l)\n",
    "\n",
    "for list_i in range(len(val_df.ndvi_before.values)):\n",
    "  df_temp = pd.Series(val_df.ndvi_before.values[list_i])\n",
    "  new_l = df_temp.fillna(method='ffill')\n",
    "  val_df.ndvi_before.values[list_i] = list(new_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVQdykQ5gcaj"
   },
   "outputs": [],
   "source": [
    "#save test df\n",
    "# save train df\n",
    "# save val df\n",
    "#NOTE: to run this, need to make sure you don't delete the \"geometry\" fields in earlier attribute lists\n",
    "\n",
    "test_df_limited=test_df[['Event_ID','geometry']].copy() #may not work when including 'Event_ID'\n",
    "test_gdf = gpd.GeoDataFrame(test_df_limited, geometry='geometry')\n",
    "test_gdf.to_file('test_df_points.shp')\n",
    "\n",
    "\n",
    "train_df_limited=train_df[['Event_ID','geometry']].copy()\n",
    "train_gdf = gpd.GeoDataFrame(train_df_limited, geometry='geometry')\n",
    "train_gdf.to_file('train_df_points.shp')\n",
    "\n",
    "\n",
    "val_df_limited=val_df[['Event_ID','geometry']].copy()\n",
    "val_gdf = gpd.GeoDataFrame(val_df_limited, geometry='geometry')\n",
    "val_gdf.to_file('val_df_points.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset out predictor columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Siv8fRyFUKD2",
    "outputId": "50fbd007-56f3-48e8-f023-a1f6dfbb3a6b"
   },
   "outputs": [],
   "source": [
    "# Read test and train data frames into numpy arrays\n",
    "\n",
    "use_these_bands = ['slope' ,'chili', 'elevation', 'aspect', 'mtpi', 'precip', \n",
    "                   'temp', 'ndvi_before','lai_before', 'severity', 'vcf_tree_before']#-> replaced w severity\n",
    "#should be same at feat_names in ablation study loop\n",
    "\n",
    "\n",
    "#should be same at feat_names in ablation study loop\n",
    "\n",
    "x_test_names_forFig = test_df['Event_ID'].values.tolist()\n",
    "\n",
    "print(pd.DataFrame(test_df[use_these_bands]).head())#.columns)\n",
    "x_test_df = test_df[use_these_bands].values.tolist()\n",
    "x_train_df = train_df[use_these_bands].values.tolist()\n",
    "x_val_df = val_df[use_these_bands].values.tolist()\n",
    "\n",
    "y_test_df1 = test_df[['ndvi_after']].values.tolist()\n",
    "y_train_df1 = train_df[['ndvi_after']].values.tolist()\n",
    "y_val_df1 = val_df[['ndvi_after']].values.tolist()\n",
    "\n",
    "y_test_df2 = test_df[['lai_after']].values.tolist()\n",
    "y_train_df2 = train_df[['lai_after']].values.tolist()\n",
    "y_val_df2 = val_df[['lai_after']].values.tolist()\n",
    "\n",
    "\n",
    "y_test_df3 = test_df[['vcf_tree_after']].values.tolist()\n",
    "y_train_df3 = train_df[['vcf_tree_after']].values.tolist()\n",
    "y_val_df3 = val_df[['vcf_tree_after']].values.tolist()\n",
    "\n",
    "test_geom = test_df[['geometry', 'Event_ID']].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv72OMOWFphJ"
   },
   "outputs": [],
   "source": [
    "#create copy of test data to store IDs\n",
    "x_test_df_storeIDs = test_df[['Event_ID']].values#.tolist() #assumes no values are removed in next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXVYtNlEjqFb"
   },
   "source": [
    "## Convert to arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIK8yx1pXj6-",
    "outputId": "057bf5d3-e155-4634-d8be-59c80696a93b"
   },
   "outputs": [],
   "source": [
    "#NOTE: as of 2/7/23, NDVI_before is the limiting factor here\n",
    "x_test_names_forFig =np.array(x_test_names_forFig)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))]\n",
    "\n",
    "X_test = np.array(x_test_df)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))]\n",
    "X_train = np.array(x_train_df)[~np.isnan(np.array(x_train_df)).any(axis=(1,2))]\n",
    "X_val = np.array(x_val_df)[~np.isnan(np.array(x_val_df)).any(axis=(1,2))]\n",
    "\n",
    "y_test1 = np.array(y_test_df1)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))]\n",
    "y_train1=  np.array(y_train_df1)[~np.isnan(np.array(x_train_df)).any(axis=(1,2))]\n",
    "y_val1 = np.array(y_val_df1)[~np.isnan(np.array(x_val_df)).any(axis=(1,2))]\n",
    "\n",
    "y_test2 = np.array(y_test_df2)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))]\n",
    "y_train2= np.array(y_train_df2)[~np.isnan(np.array(x_train_df)).any(axis=(1,2))]\n",
    "y_val2 = np.array(y_val_df2)[~np.isnan(np.array(x_val_df)).any(axis=(1,2))]\n",
    "\n",
    "\n",
    "y_test3, y_train3, y_val3 = np.array(y_test_df3)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))], np.array(y_train_df3)[~np.isnan(np.array(x_train_df)).any(axis=(1,2))],np.array(y_val_df3)[~np.isnan(np.array(x_val_df)).any(axis=(1,2))]\n",
    "\n",
    "test_geom = np.array(test_geom)[~np.isnan(np.array(x_test_df)).any(axis=(1,2))]\n",
    "\n",
    "#reformat labels\n",
    "x_test_list_storeIDs = np.array(x_test_df_storeIDs)#[~np.isnan(np.array(x_test_df_storeIDs)).any(axis=(1,2))]\n",
    "print(\"label shp:\", x_test_list_storeIDs.shape)\n",
    "print('X Val:',X_val.shape, 'y val:',y_val1.shape)\n",
    "print('X_train:',X_train.shape, 'y train:', y_train1.shape)\n",
    "print('X Test:',X_test.shape, 'y test:',y_test1.shape)\n",
    "print(\"Note: shape format is #samples, #features, # of timesteps\\n\")\n",
    "\n",
    "tot = y_train1.shape[0]+y_test1.shape[0]+y_val1.shape[0]\n",
    "print(\"train/val/test point split\",y_train1.shape[0]/tot,\"/\",y_val1.shape[0]/tot,\"/\",y_test1.shape[0]/tot )\n",
    "\n",
    "#check: have we removed points?\n",
    "print(\"prev shape of test was:\",x_test_df_storeIDs.shape) #first shape 0 should match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0,9,:], color='red', label = \"vcf\")\n",
    "plt.plot(X_test[0,10,:], label=\"severity \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train2[80][0], color='green')\n",
    "\n",
    "plt.plot(y_test2[7][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NRvXOh8HB0W"
   },
   "source": [
    "## Remove 255s; divide by 10k; Standardize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace lai values of 255 with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,8,:][X_train[:,8,:] == 255] = 0\n",
    "X_val[:,8,:][X_val[:,8,:] == 255] = 0\n",
    "X_test[:,8,:][X_test[:,8,:] == 255] = 0\n",
    "\n",
    "y_train2[:,0,:][y_train2[:,0,:] == 255] = 0\n",
    "y_test2[:,0,:][y_test2[:,0,:] == 255] = 0\n",
    "y_val2[:,0,:][y_val2[:,0,:] == 255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide x_train_ndvi by 10k \n",
    "To get to original value before GEE data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,7,:] = X_train[:,7,:]/10000\n",
    "X_test[:,7,:] = X_test[:,7,:]/10000\n",
    "X_val[:,7,:] = X_val[:,7,:]/10000\n",
    "\n",
    "y_train1 = y_train1/10000\n",
    "y_test1 = y_test1/10000\n",
    "y_val1 = y_val1/10000\n",
    "\n",
    "\n",
    "X_train[:,8,:] = X_train[:,8,:]*0.1\n",
    "X_test[:,8,:] = X_test[:,8,:]*0.1\n",
    "X_val[:,8,:] = X_val[:,8,:]*0.1\n",
    "\n",
    "y_train2 = y_train2*0.1\n",
    "y_test2 = y_test2*0.1\n",
    "y_val2 = y_val2*0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1s7FPq6l2Cic",
    "outputId": "1ac68b72-e9d4-41f3-c2a5-7ec2e5c38f84"
   },
   "outputs": [],
   "source": [
    "# Standardize data by hand because StandarScalar doesn't take multidimensional rasters\n",
    "#NOTE this is for when we have three target vars ********************\n",
    "\n",
    "n_samples, n_feats, n_tsteps =  X_train.shape\n",
    "\n",
    "X_train = np.transpose(X_train,axes=(0,2,1))#reshape(n_samples, n_tsteps, n_feats)\n",
    "print(X_train.shape,\"is new shp\")\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "mean_value = np.mean(X_train, axis= (tuple(range(X_train.ndim-1))))\n",
    "std_val = np.std(X_train, axis= (tuple(range(X_train.ndim-1))))\n",
    "print(\"NDVI mean\", mean_value[7])\n",
    "print(\"LAI mean\", mean_value[8])\n",
    "print(\"NDVI std\", std_val[7])\n",
    "print(\"lai std\", std_val[8])\n",
    "\n",
    "\n",
    "\n",
    "print(\"orig min of ndvi:\", np.min(X_train,axis= (tuple(range(X_train.ndim-1))))[7])\n",
    "print(\"orig max of ndvi:\", np.max(X_train,axis= (tuple(range(X_train.ndim-1))))[7])\n",
    "\n",
    "print(\"orig min of lai:\", np.min(X_train,axis= (tuple(range(X_train.ndim-1))))[8])\n",
    "print(\"orig max of lai:\", np.max(X_train,axis= (tuple(range(X_train.ndim-1))))[8])\n",
    "\n",
    "\n",
    "#subtract mean and then divide by stdev\n",
    "\n",
    "X_test = np.transpose(X_test,axes=(0,2,1))#reshape(n_samples, n_tsteps, n_feats)\n",
    "X_val = np.transpose(X_val,axes=(0,2,1))#reshape(n_samples, n_tsteps, n_feats)\n",
    "\n",
    "y_train1 = np.transpose(y_train1,axes=(0,2,1))\n",
    "y_test1 =np.transpose(y_test1,axes=(0,2,1))\n",
    "y_val1 = np.transpose(y_val1,axes=(0,2,1))\n",
    "\n",
    "y_train2 = np.transpose(y_train2,axes=(0,2,1))\n",
    "y_test2=np.transpose(y_test2,axes=(0,2,1))\n",
    "y_val2 = np.transpose(y_val2,axes=(0,2,1))\n",
    "\n",
    "y_train3 = np.transpose(y_train3,axes=(0,2,1))\n",
    "y_test3=np.transpose(y_test3,axes=(0,2,1))\n",
    "y_val3 = np.transpose(y_val3,axes=(0,2,1))\n",
    "\n",
    "\n",
    "print(y_test1.shape, \"is post transpose y shape\")\n",
    "\n",
    "# min_lai = np.min(X_train[:,:,8])\n",
    "# max_lai = np.max(X_train[:,:,8])\n",
    "\n",
    "# X_train[:,:,8] = (X_train[:,:,8] - min_lai)/ (max_lai - min_lai)\n",
    "# X_val[:,:,8] = (X_val[:,:,8] - min_lai)/ (max_lai - min_lai)\n",
    "# X_test[:,:,8] = (X_test[:,:,8] - min_lai)/ (max_lai - min_lai)\n",
    "X_train = (X_train - mean_value)/std_val\n",
    "X_test = (X_test - mean_value)/std_val\n",
    "X_val = (X_val - mean_value)/std_val\n",
    "\n",
    "mean_value_ndvi = mean_value[7] ## prev: [-3]#  mean_value[-1]\n",
    "mean_value_lai = mean_value[8]# prev: -2\n",
    "mean_value_vcf = mean_value[9]\n",
    "# print(mean_value.shape,\"is new shape of mean value\")\n",
    "std_val_ndvi = std_val[7]\n",
    "std_val_lai = std_val[8]\n",
    "std_val_vcf = std_val[9]\n",
    "\n",
    "\n",
    "# print('\\n\\nndvi mean in x train:',mean_value)\n",
    "y_train1 = (y_train1 - mean_value_ndvi)/std_val_ndvi\n",
    "# y_test1 = (y_test1 - mean_value_ndvi)/std_val_ndvi\n",
    "y_val1 = (y_val1 - mean_value_ndvi)/std_val_ndvi\n",
    "\n",
    "y_train2 = (y_train2 - mean_value_lai)/std_val_lai\n",
    "# y_test2 = (y_test2 - mean_value_lai)/std_val_lai\n",
    "y_val2 = (y_val2 - mean_value_lai)/std_val_lai\n",
    "\n",
    "y_train3 = (y_train3 - mean_value_vcf)/std_val_vcf\n",
    "# y_test3 = (y_test3 - mean_value_vcf)/std_val_vcf\n",
    "y_val3 = (y_val3 - mean_value_vcf)/std_val_vcf\n",
    "print(y_test1.shape, \"is output y shape\")\n",
    "print(\"Note: shape format is now #samples,  # of timesteps, #features or targets in case of y,\\n\")\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Now min/ax of lai is:\", np.min(y_train2.flatten()), np.max(y_train2.flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "179DukVTcb1n"
   },
   "source": [
    "## Final Reshaping of data to shape/format: (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop VCF as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#final reshaping for multihead model\n",
    "# You can stack the y_train and y_test variables as 1-d arrays originally as follows:\n",
    "y_train = np.dstack((y_train1, y_train2))#, y_train3))  # 3D numpy array (note chat wanted column_stack)\n",
    "y_val = np.dstack((y_val1, y_val2))#, y_val3))\n",
    "y_test = np.dstack((y_test1, y_test2))#, y_test3))\n",
    "\n",
    "print(y_test1.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test (graph) viz of variables to ensure theyre still on same scale as each other after standardization process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data as .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save( \"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_val.npy\", y_val)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "np.save(\"y_train1.npy\", y_train1)\n",
    "np.save(\"y_train2.npy\", y_train2)\n",
    "np.save(\"y_test1.npy\", y_test1)\n",
    "np.save(\"y_test2.npy\", y_test2)\n",
    "np.save(\"y_val1.npy\", y_val1)\n",
    "np.save(\"y_val2.npy\", y_val2) #note: saving stdardized data so will need ot unstd. to get back to real val range\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3r6q8GBGjjp0",
    "7avilUEjjqM1",
    "vQWjIbhMjqVX",
    "-XvBsJj2ua9c",
    "piZtfbsSugBQ"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "598px",
    "width": "509px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
